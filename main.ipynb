{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bbfeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import flatten\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from scipy.io import loadmat\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a8dcba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=102):\n",
    "        super(FlowerClassifier,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2 ,stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(self.relu(self.bn5(self.conv5(x))))\n",
    "\n",
    "        x = self.flatten(x)  \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f915a90-1f62-4317-a946-26da3fd7192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowerClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31a2a100-1f18-493e-adcb-85d8fb171c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eab5be8-0ec2-4889-bfdb-a0456e132b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = datasets.Flowers102(root='./data', split='train', transform=trainTransform, download=True)\n",
    "val_dataset = datasets.Flowers102(root='./data', split='val', transform=testTransform, download=True)\n",
    "test_dataset = datasets.Flowers102(root='./data', split='test', transform=testTransform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchSize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66b6e86c-6749-4c7d-a060-ec2d52708dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bddd53-ac58-4259-8074-e17526cc5b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Train Loss: 12.1491, Train Accuracy: 1.08%\n",
      "Validation Loss: 6.1024, Validation Accuracy: 0.98%\n",
      "Epoch [2/20]\n",
      "Train Loss: 6.0360, Train Accuracy: 0.88%\n",
      "Validation Loss: 5.0222, Validation Accuracy: 1.47%\n",
      "Epoch [3/20]\n",
      "Train Loss: 5.1145, Train Accuracy: 1.86%\n",
      "Validation Loss: 4.7496, Validation Accuracy: 0.98%\n",
      "Epoch [4/20]\n",
      "Train Loss: 4.7583, Train Accuracy: 1.47%\n",
      "Validation Loss: 4.5923, Validation Accuracy: 2.45%\n",
      "Epoch [5/20]\n",
      "Train Loss: 4.5867, Train Accuracy: 1.57%\n",
      "Validation Loss: 4.4555, Validation Accuracy: 2.06%\n",
      "Epoch [6/20]\n",
      "Train Loss: 4.4991, Train Accuracy: 1.76%\n",
      "Validation Loss: 4.3971, Validation Accuracy: 2.25%\n",
      "Epoch [7/20]\n",
      "Train Loss: 4.4039, Train Accuracy: 3.14%\n",
      "Validation Loss: 4.3055, Validation Accuracy: 3.53%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(\"Epoch [\" + str(epoch + 1) + \"/\" + str(num_epochs) + \"]\")\n",
    "    print(\"Train Loss: {:.4f}, Train Accuracy: {:.2f}%\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f}, Validation Accuracy: {:.2f}%\".format(val_loss, val_acc))\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18d9e8-7df0-4783-b56f-47178d204f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correct = 0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f3412-c145-4162-b8ce-f4a2e5b13ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
